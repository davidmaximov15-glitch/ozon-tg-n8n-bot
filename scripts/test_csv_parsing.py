#!/usr/bin/env python3
"""
Test CSV parsing logic against real FBO/FBS data files.
Identifies issues with column mapping, date parsing, and statistics calculation.
"""

import csv
import json
from datetime import datetime, timedelta
from collections import defaultdict
import sys

def parse_fbo_date(date_str):
    """Parse FBO date format: 01.10.2025 7:26 -> UTC datetime"""
    try:
        # FBO format: DD.MM.YYYY H:MM (without leading zeros for hours)
        dt = datetime.strptime(date_str.strip(), "%d.%m.%Y %H:%M")
        return dt
    except:
        try:
            # Try with seconds
            dt = datetime.strptime(date_str.strip(), "%d.%m.%Y %H:%M:%S")
            return dt
        except Exception as e:
            print(f"âŒ Failed to parse FBO date '{date_str}': {e}")
            return None

def parse_fbs_date(date_str):
    """Parse FBS date format: 2025-09-27 21:10:51 -> UTC datetime"""
    try:
        dt = datetime.strptime(date_str.strip(), "%Y-%m-%d %H:%M:%S")
        return dt
    except Exception as e:
        print(f"âŒ Failed to parse FBS date '{date_str}': {e}")
        return None

def utc_to_msk(dt):
    """Convert UTC datetime to MSK (UTC+3)"""
    return dt + timedelta(hours=3)

def analyze_fbo_csv(filepath):
    """Analyze FBO CSV structure and data"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ANALYZING FBO CSV: {filepath}")
    print(f"{'='*80}\n")
    
    with open(filepath, 'r', encoding='utf-8-sig') as f:
        # Read with semicolon delimiter
        reader = csv.DictReader(f, delimiter=';')
        
        # Print header analysis
        print("ğŸ“‹ HEADER COLUMNS:")
        headers = reader.fieldnames
        for idx, col in enumerate(headers):
            print(f"  [{chr(65+idx) if idx < 26 else 'A'+chr(65+idx-26)}] Column {idx}: {col[:50]}")
        
        # Process rows
        rows = list(reader)
        print(f"\nğŸ“¦ Total rows: {len(rows)}")
        
        # Analyze first few rows
        print("\nğŸ” FIRST 3 ROWS ANALYSIS:")
        for i, row in enumerate(rows[:3]):
            print(f"\n  Row {i+1}:")
            print(f"    ĞĞ¾Ğ¼ĞµÑ€ Ğ·Ğ°ĞºĞ°Ğ·Ğ° (A): {row.get('ĞĞ¾Ğ¼ĞµÑ€ Ğ·Ğ°ĞºĞ°Ğ·Ğ°', 'N/A')}")
            print(f"    ĞÑ€Ñ‚Ğ¸ĞºÑƒĞ» (L): {row.get('ĞÑ€Ñ‚Ğ¸ĞºÑƒĞ»', 'N/A')}")
            print(f"    Ğ’Ğ°ÑˆĞ° Ñ†ĞµĞ½Ğ° (M): {row.get('Ğ’Ğ°ÑˆĞ° Ñ†ĞµĞ½Ğ°', 'N/A')}")
            print(f"    ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ (Q): {row.get('ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾', 'N/A')}")
            print(f"    Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ (E): {row.get('Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ', 'N/A')}")
            print(f"    ĞŸÑ€Ğ¸Ğ½ÑÑ‚ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ (C): {row.get('ĞŸÑ€Ğ¸Ğ½ÑÑ‚ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ', 'N/A')}")
        
        # Test date parsing
        print("\nğŸ“… DATE PARSING TEST:")
        dates_parsed = []
        dates_failed = []
        for row in rows[:10]:
            date_str = row.get('ĞŸÑ€Ğ¸Ğ½ÑÑ‚ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ', '')
            dt_utc = parse_fbo_date(date_str)
            if dt_utc:
                dt_msk = utc_to_msk(dt_utc)
                dates_parsed.append(dt_msk.strftime("%Y-%m-%d"))
                print(f"  âœ… '{date_str}' -> UTC: {dt_utc} -> MSK: {dt_msk.strftime('%Y-%m-%d %H:%M')}")
            else:
                dates_failed.append(date_str)
        
        if dates_failed:
            print(f"\n  âŒ Failed to parse {len(dates_failed)} dates")
        
        # Extract unique dates
        unique_dates = sorted(set(dates_parsed))
        print(f"\n  ğŸ“… Unique dates found: {unique_dates}")
        
        # Test statistics calculation
        print("\nğŸ“Š STATISTICS CALCULATION TEST:")
        stats = defaultdict(lambda: {'orders': 0, 'cancellations': 0, 'revenue': 0, 'price_sum': 0, 'price_qty': 0})
        
        STATUS_REVENUE = {'Ğ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½', 'Ğ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ', 'Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµÑ‚ ÑĞ±Ğ¾Ñ€ĞºĞ¸', 'Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ³Ñ€ÑƒĞ·ĞºĞ¸'}
        STATUS_CANCEL = {'Ğ¾Ñ‚Ğ¼ĞµĞ½Ñ‘Ğ½', 'Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚'}
        
        for row in rows[:20]:
            sku = row.get('ĞÑ€Ñ‚Ğ¸ĞºÑƒĞ»', '').strip()
            if not sku:
                continue
            
            try:
                quantity = int(row.get('ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾', '1'))
            except:
                quantity = 1
            
            try:
                price_str = row.get('Ğ’Ğ°ÑˆĞ° Ñ†ĞµĞ½Ğ°', '0').replace(',', '.')
                price = float(price_str)
            except:
                price = 0
            
            status = row.get('Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ', '').lower().strip()
            
            if status in STATUS_REVENUE:
                stats[sku]['orders'] += quantity
                stats[sku]['price_sum'] += price * quantity
                stats[sku]['price_qty'] += quantity
                stats[sku]['revenue'] += price * quantity
            
            if status in STATUS_CANCEL:
                stats[sku]['cancellations'] += quantity
        
        print(f"\n  Found {len(stats)} unique SKUs in first 20 rows:")
        for sku, data in list(stats.items())[:5]:
            avg_price = data['price_sum'] / data['price_qty'] if data['price_qty'] > 0 else 0
            print(f"    {sku}:")
            print(f"      Orders: {data['orders']}, Cancellations: {data['cancellations']}")
            print(f"      Avg Price: {avg_price:.2f}, Revenue: {data['revenue']:.2f}")

def analyze_fbs_csv(filepath):
    """Analyze FBS CSV structure and data"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ANALYZING FBS CSV: {filepath}")
    print(f"{'='*80}\n")
    
    with open(filepath, 'r', encoding='utf-8-sig') as f:
        # Read with semicolon delimiter and quoted fields
        reader = csv.DictReader(f, delimiter=';', quotechar='"')
        
        # Print header analysis
        print("ğŸ“‹ HEADER COLUMNS:")
        headers = reader.fieldnames
        for idx, col in enumerate(headers):
            print(f"  [{chr(65+idx) if idx < 26 else 'A'+chr(65+idx-26)}] Column {idx}: {col[:50]}")
        
        # Process rows
        rows = list(reader)
        print(f"\nğŸ“¦ Total rows: {len(rows)}")
        
        # Analyze first few rows
        print("\nğŸ” FIRST 3 ROWS ANALYSIS:")
        for i, row in enumerate(rows[:3]):
            print(f"\n  Row {i+1}:")
            print(f"    ĞĞ¾Ğ¼ĞµÑ€ Ğ·Ğ°ĞºĞ°Ğ·Ğ°: {row.get('ĞĞ¾Ğ¼ĞµÑ€ Ğ·Ğ°ĞºĞ°Ğ·Ğ°', 'N/A')}")
            print(f"    ĞÑ€Ñ‚Ğ¸ĞºÑƒĞ»: {row.get('ĞÑ€Ñ‚Ğ¸ĞºÑƒĞ»', 'N/A')}")
            print(f"    Ğ’Ğ°ÑˆĞ° Ñ†ĞµĞ½Ğ°: {row.get('Ğ’Ğ°ÑˆĞ° Ñ†ĞµĞ½Ğ°', 'N/A')}")
            print(f"    ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾: {row.get('ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾', 'N/A')}")
            print(f"    Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ: {row.get('Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ', 'N/A')}")
            print(f"    ĞŸÑ€Ğ¸Ğ½ÑÑ‚ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ: {row.get('ĞŸÑ€Ğ¸Ğ½ÑÑ‚ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ', 'N/A')}")
        
        # Test date parsing
        print("\nğŸ“… DATE PARSING TEST:")
        dates_parsed = []
        dates_failed = []
        for row in rows[:10]:
            date_str = row.get('ĞŸÑ€Ğ¸Ğ½ÑÑ‚ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ', '')
            dt_utc = parse_fbs_date(date_str)
            if dt_utc:
                dt_msk = utc_to_msk(dt_utc)
                dates_parsed.append(dt_msk.strftime("%Y-%m-%d"))
                print(f"  âœ… '{date_str}' -> UTC: {dt_utc} -> MSK: {dt_msk.strftime('%Y-%m-%d %H:%M')}")
            else:
                dates_failed.append(date_str)
        
        if dates_failed:
            print(f"\n  âŒ Failed to parse {len(dates_failed)} dates")
        
        # Extract unique dates
        unique_dates = sorted(set(dates_parsed))
        print(f"\n  ğŸ“… Unique dates found: {unique_dates}")

def main():
    print("ğŸš€ CSV PARSING TEST SUITE")
    print("Testing real FBO/FBS data against specification requirements")
    
    # Test FBO
    analyze_fbo_csv('orders-2025-fbo-test.csv')
    
    # Test FBS
    analyze_fbs_csv('orders-2025-fbs-test.csv')
    
    print(f"\n{'='*80}")
    print("âœ… ANALYSIS COMPLETE")
    print(f"{'='*80}\n")

if __name__ == '__main__':
    main()
